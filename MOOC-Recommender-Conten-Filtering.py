# -*- coding: utf-8 -*-
"""Copy of Content Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10yimuvRJzEHfR0y9QGrXYKvZHryvrJYc
"""

#importing the libraries
import pandas as pd
import numpy as np
import networkx as nx
from mlxtend.frequent_patterns import apriori,association_rules
import plotly.express as px
import plotly.graph_objects as go
from mlxtend.preprocessing import TransactionEncoder
import glob
plt.style.use('default')

# Mounting the file on the drive
from google.colab import drive
drive.mount('/content/drive')

def MID(s, start, length):
  """
  modifying the names of the courses
  example: MID("apple",2,3) returns "ppl"
  """

  return str(s[start - 1: start - 1 + length])


#creating the list of all the courses

df_list = []
#obtaining all the files from the folder
for file in glob.glob(r'/content/drive/MyDrive/Hackathon/2020 - Jan/*.csv'):
    # Tweak this to work for your actual filepaths, if needed.

    t_course_id = MID(file,56,10)
    #calling the MID function to modify the file name
    df = (pd.read_csv(file)
            .assign(Course_id=t_course_id))

    df_list.append(df)


df = pd.concat(df_list)

# This the same code except
def MID(s, start, length):
  """
  modifying the names of the courses
  example: MID("apple",2,3) returns "ppl"
  """
  return str(s[start - 1: start - 1 + length])




df_list = []
for file in glob.glob(r'/content/drive/MyDrive/Hackathon/2020 - July/*.csv'):
    # Tweak this to work for your actual filepaths, if needed.

    t_course_id = MID(file,57,10)
    #print(course_id)
    df = (pd.read_csv(file)
            .assign(Course_id=t_course_id))


    df_list.append(df)

df1 = pd.concat(df_list)

df2=pd.concat([df,df1])

l=list([1]*len(df2))

df2["e"]=l

df2.head()

df2.isnull().sum()

len(df2['user_id'].unique())

len(df2['Course_id'].unique())

df_new=df2.groupby('user_id')['Course_id'].count()

table = pd.pivot_table(df2,index=['user_id'],values=['Course_id'], aggfunc=pd.Series.nunique)

table.head()

table=table.drop(table[table['Course_id']<2].index,axis=0)

table=table.drop(table[table['Course_id']>10].index,axis=0)

len(table)

df_1 = pd.merge(df2, table, on='user_id', how='inner')

df_1.head()

len(df_1['user_id'].unique())

column= df_1[["user_id","Course_id_x","e"]]
column.head()

column.shape

column.isnull().sum()

column1=column.drop_duplicates()

Final = column1.pivot(index=['user_id'], columns='Course_id_x', values='e')

Final.isnull().sum()

#Final.to_csv("FINAL_FILE.csv")



Final.head()

Final.fillna(0,inplace= True)

frequent_itemsets = apriori(Final, min_support=0.01, use_colnames=True)
frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))
frequent_itemsets

frequent_itemsets.to_csv("Freq2020.csv")

